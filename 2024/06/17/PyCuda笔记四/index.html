<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="背景本文主要记录《Hands-On GPU Programming with Python and CUDA 》第6章的内容，这里主要介绍了如何调试cuda代码，C语言Cuda相关，性能分析等知识。 第六章使用printfCUDA 内核函数支持 printf 是从 CUDA 3.2 版本开始引入的。 这个功能允许在 GPU 代码中使用 printf 函数进行调试和输出，这在调试 CUDA 内核时非">
<meta property="og:type" content="article">
<meta property="og:title" content="PyCuda笔记四">
<meta property="og:url" content="http://example.com/2024/06/17/PyCuda%E7%AC%94%E8%AE%B0%E5%9B%9B/index.html">
<meta property="og:site_name" content="Cat YuanBao">
<meta property="og:description" content="背景本文主要记录《Hands-On GPU Programming with Python and CUDA 》第6章的内容，这里主要介绍了如何调试cuda代码，C语言Cuda相关，性能分析等知识。 第六章使用printfCUDA 内核函数支持 printf 是从 CUDA 3.2 版本开始引入的。 这个功能允许在 GPU 代码中使用 printf 函数进行调试和输出，这在调试 CUDA 内核时非">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="article:published_time" content="2024-06-17T01:34:40.000Z">
<meta property="article:modified_time" content="2024-06-17T01:34:57.079Z">
<meta property="article:author" content="橘猫元宝">
<meta property="article:tag" content="cat">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/404.jpg">

    <meta name="keywords" content="cat">


<title >PyCuda笔记四</title>

<!-- Favicon -->

    <link href='/img/favicon.svg?v=2.1.8' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/img/favicon.svg?v=2.1.8' rel='icon' type='image/png' sizes='32x32' ></link>




<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    




<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"橘猫元宝","root":"/","typed_text":null,"theme_version":"2.1.8","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"/img/favicon.svg","icon16":"/img/favicon.svg","icon32":"/img/favicon.svg","appleTouchIcon":null,"webmanifest":null,"visibilitychange":false,"hidden":"/failure.ico","showText":"(/≧▽≦/)咦！又好了！","hideText":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://unpkg.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","user_tag":"fas fa-user-alt","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"prismjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2024-06-17 09:34:57"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.1.8" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->
 
<meta name="generator" content="Hexo 7.0.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/img/favicon.svg">
    
    
        <div class="trm-logo-text">
            元宝<span>橘猫</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    归档
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="/img/banner.png">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            NEWS LETTER
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            PyCuda笔记四
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2024
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/img/cat.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        橘猫元宝
    </h5>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com" title="Github" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                Residence:
            </div>
            <div class="trm-label trm-label-light">
                Mars
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            06/17
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            09:34
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            橘猫元宝
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>本文主要记录《Hands-On GPU Programming with Python and CUDA 》第6章的内容，这里主要介绍了如何调试cuda代码，C语言Cuda相关，性能分析等知识。</p>
<h3 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h3><h4 id="使用printf"><a href="#使用printf" class="headerlink" title="使用printf"></a>使用<code>printf</code></h4><p>CUDA 内核函数支持 <code>printf</code> 是从 CUDA 3.2 版本开始引入的。</p>
<p>这个功能允许在 GPU 代码中使用 <code>printf</code> 函数进行调试和输出，这在调试 CUDA 内核时非常有用。</p>
<p>使用 <code>printf</code> 的注意事项</p>
<ol>
<li><strong>性能开销</strong>：在内核中使用 <code>printf</code> 会有一定的性能开销，因为它需要同步内核和主机之间的数据。</li>
<li><strong>输出限制</strong>：输出的字符数量是有限的。如果输出的字符超过限制，后续的输出会被截断。</li>
<li><strong>调试用途</strong>：一般来说，<code>printf</code> 主要用于调试，不推荐在生产代码中大量使用。</li>
</ol>
<p>实例：</p>
<pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>

__global__ <span class="token keyword">void</span> <span class="token function">my_kernel</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">int</span> idx <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Thread %d\n"</span><span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    my_kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment" spellcheck="true">// 等待核函数结束，在此之前会阻塞。</span>
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>书中实例：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit
<span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>driver <span class="token keyword">as</span> drv
<span class="token keyword">from</span> pycuda <span class="token keyword">import</span> gpuarray
<span class="token keyword">from</span> pycuda<span class="token punctuation">.</span>compiler <span class="token keyword">import</span> SourceModule

ker <span class="token operator">=</span> SourceModule<span class="token punctuation">(</span><span class="token triple-quoted-string string">'''
__global__ void hello_world_ker()
{
        printf("Hello world from thread %d, in block %d!\\n", threadIdx.x, bloc
kIdx.x);

        __syncthreads();

        if(threadIdx.x == 0 &amp;&amp; blockIdx.x == 0)
        {
                printf("-------------------------------------\\n");
                printf("This kernel was launched over a grid consisting of %d b
locks,\\n", gridDim.x);
                printf("where each block has %d threads.\\n", blockDim.x);
        }
}
'''</span><span class="token punctuation">)</span>

hello_ker <span class="token operator">=</span> ker<span class="token punctuation">.</span>get_function<span class="token punctuation">(</span><span class="token string">"hello_world_ker"</span><span class="token punctuation">)</span>
hello_ker<span class="token punctuation">(</span>block<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> grid<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在代码中，<code>block=(5,1,1)</code> 和 <code>grid=(2,1,1)</code> 的含义是：</p>
<ul>
<li><strong>Block</strong>：每个块包含 5 个线程（线程数为 <code>blockDim.x = 5</code>，<code>blockDim.y = 1</code>，<code>blockDim.z = 1</code>）。</li>
<li><strong>Grid</strong>：网格包含 2 个块（块数为 <code>gridDim.x = 2</code>，<code>gridDim.y = 1</code>，<code>gridDim.z = 1</code>）。</li>
</ul>
<p>一个测试C代码的小程序:</p>
<pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h></span></span>
<span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>

__global__ <span class="token keyword">void</span> <span class="token function">divergence_test_ker</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span> threadIdx<span class="token punctuation">.</span>x <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
                <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"threadIdx.x %d : This is an even thread.\n"</span><span class="token punctuation">,</span> threadIdx<span class="token punctuation">.</span>
x<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">else</span>
                <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"threadIdx.x %d : This is an odd thread.\n"</span><span class="token punctuation">,</span> threadIdx<span class="token punctuation">.</span>x
<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

__host__ <span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        divergence_test_ker<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">cudaDeviceReset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>运行结果：</p>
<pre class="line-numbers language-bash"><code class="language-bash">$ ./test
threadIdx.x 1 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 3 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 5 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 7 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 9 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 11 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 13 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 15 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 17 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 19 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 21 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 23 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 25 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 27 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 29 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 31 <span class="token keyword">:</span> This is an odd thread.
threadIdx.x 0 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 2 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 4 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 6 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 8 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 10 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 12 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 14 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 16 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 18 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 20 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 22 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 24 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 26 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 28 <span class="token keyword">:</span> This is an even thread.
threadIdx.x 30 <span class="token keyword">:</span> This is an even thread.
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>C语言矩阵相关的实例代码：</p>
<pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h></span></span>
<span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>
<span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdlib.h></span></span>
<span class="token macro property">#<span class="token directive keyword">define</span> _EPSILON 0.001</span>
<span class="token macro property">#<span class="token directive keyword">define</span> _ABS(x) ( x > 0.0f ? x : -x )</span>

__host__ <span class="token keyword">int</span> <span class="token function">allclose</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token keyword">int</span> len<span class="token punctuation">)</span>
<span class="token punctuation">{</span>

        <span class="token keyword">int</span> returnval <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> len<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
        <span class="token punctuation">{</span>
                <span class="token keyword">if</span> <span class="token punctuation">(</span> <span class="token function">_ABS</span><span class="token punctuation">(</span>A<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">></span> _EPSILON <span class="token punctuation">)</span>
                <span class="token punctuation">{</span>
                        returnval <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>
                        <span class="token keyword">break</span><span class="token punctuation">;</span>
                <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>

        <span class="token keyword">return</span><span class="token punctuation">(</span>returnval<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>


<span class="token comment" spellcheck="true">// row-column dot-product for matrix multiplication</span>
__device__ <span class="token keyword">float</span> <span class="token function">rowcol_dot</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>matrix_a<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>matrix_b<span class="token punctuation">,</span> <span class="token keyword">int</span> row<span class="token punctuation">,</span> <span class="token keyword">int</span> col<span class="token punctuation">,</span>
 <span class="token keyword">int</span> N<span class="token punctuation">)</span>
<span class="token punctuation">{</span>
        <span class="token keyword">float</span> val <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span>
        <span class="token punctuation">{</span>
        val <span class="token operator">+</span><span class="token operator">=</span> matrix_a<span class="token punctuation">[</span> row<span class="token operator">*</span>N <span class="token operator">+</span> k <span class="token punctuation">]</span> <span class="token operator">*</span> matrix_b<span class="token punctuation">[</span> col <span class="token operator">+</span> k<span class="token operator">*</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token keyword">return</span><span class="token punctuation">(</span>val<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token comment" spellcheck="true">// matrix multiplication kernel that is parallelized over row/column tuples.</span>
__global__ <span class="token keyword">void</span> <span class="token function">matrix_mult_ker</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span> matrix_a<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span> matrix_b<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span> out
put_matrix<span class="token punctuation">,</span> <span class="token keyword">int</span> N<span class="token punctuation">)</span>
<span class="token punctuation">{</span>

    <span class="token keyword">int</span> row <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">int</span> col <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>y<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>

        output_matrix<span class="token punctuation">[</span>col <span class="token operator">+</span> row<span class="token operator">*</span>N<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">rowcol_dot</span><span class="token punctuation">(</span>matrix_a<span class="token punctuation">,</span> matrix_b<span class="token punctuation">,</span> row<span class="token punctuation">,</span> col<span class="token punctuation">,</span> N
<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>


__host__ <span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span>

        <span class="token comment" spellcheck="true">// Initialize to use first GPU.</span>
        <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// this indicates the width/height of the matrices</span>
        <span class="token keyword">int</span> N <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// this will indicate how many bytes to allocate to store a test or out</span>
put matrix
        <span class="token keyword">int</span> num_bytes <span class="token operator">=</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>N<span class="token operator">*</span>N<span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// input test matrix A</span>
        <span class="token keyword">float</span> h_A<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token number">1.0</span><span class="token punctuation">,</span>  <span class="token number">2.0</span><span class="token punctuation">,</span>  <span class="token number">3.0</span><span class="token punctuation">,</span>  <span class="token number">4.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">1.0</span><span class="token punctuation">,</span>  <span class="token number">2.0</span><span class="token punctuation">,</span>  <span class="token number">3.0</span><span class="token punctuation">,</span>  <span class="token number">4.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">1.0</span><span class="token punctuation">,</span>  <span class="token number">2.0</span><span class="token punctuation">,</span>  <span class="token number">3.0</span><span class="token punctuation">,</span>  <span class="token number">4.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">1.0</span><span class="token punctuation">,</span>  <span class="token number">2.0</span><span class="token punctuation">,</span>  <span class="token number">3.0</span><span class="token punctuation">,</span>  <span class="token number">4.0</span> <span class="token punctuation">}</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// input test matrix B</span>
        <span class="token keyword">float</span> h_B<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token number">14.0</span><span class="token punctuation">,</span>  <span class="token number">13.0</span><span class="token punctuation">,</span>  <span class="token number">12.0</span><span class="token punctuation">,</span>  <span class="token number">11.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">14.0</span><span class="token punctuation">,</span>  <span class="token number">13.0</span><span class="token punctuation">,</span>  <span class="token number">12.0</span><span class="token punctuation">,</span>  <span class="token number">11.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">14.0</span><span class="token punctuation">,</span>  <span class="token number">13.0</span><span class="token punctuation">,</span>  <span class="token number">12.0</span><span class="token punctuation">,</span>  <span class="token number">11.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">14.0</span><span class="token punctuation">,</span>  <span class="token number">13.0</span><span class="token punctuation">,</span>  <span class="token number">12.0</span><span class="token punctuation">,</span>  <span class="token number">11.0</span> <span class="token punctuation">}</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// expected output of A times B</span>
        <span class="token keyword">float</span> h_AxB<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token number">140.0</span><span class="token punctuation">,</span>  <span class="token number">130.0</span><span class="token punctuation">,</span>  <span class="token number">120.0</span><span class="token punctuation">,</span>  <span class="token number">110.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">140.0</span><span class="token punctuation">,</span>  <span class="token number">130.0</span><span class="token punctuation">,</span>  <span class="token number">120.0</span><span class="token punctuation">,</span>  <span class="token number">110.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">140.0</span><span class="token punctuation">,</span>  <span class="token number">130.0</span><span class="token punctuation">,</span>  <span class="token number">120.0</span><span class="token punctuation">,</span>  <span class="token number">110.0</span><span class="token punctuation">,</span> \
                                        <span class="token number">140.0</span><span class="token punctuation">,</span>  <span class="token number">130.0</span><span class="token punctuation">,</span>  <span class="token number">120.0</span><span class="token punctuation">,</span>  <span class="token number">110.0</span> <span class="token punctuation">}</span><span class="token punctuation">;</span>


        <span class="token comment" spellcheck="true">// these pointers will be used for the GPU.</span>
        <span class="token comment" spellcheck="true">// (notice how we use normal float pointers)</span>
        <span class="token keyword">float</span> <span class="token operator">*</span> d_A<span class="token punctuation">;</span>
        <span class="token keyword">float</span> <span class="token operator">*</span> d_B<span class="token punctuation">;</span>
        <span class="token keyword">float</span> <span class="token operator">*</span> d_output<span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// allocate memory for the test matrices on the GPU</span>
        <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>d_A<span class="token punctuation">,</span> num_bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>d_B<span class="token punctuation">,</span> num_bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// copy the test matrices to the GPU</span>
        <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_A<span class="token punctuation">,</span> h_A<span class="token punctuation">,</span> num_bytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_B<span class="token punctuation">,</span> h_B<span class="token punctuation">,</span> num_bytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// allocate memory for output on GPU</span>
        <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>d_output<span class="token punctuation">,</span> num_bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// this will store the output from the GPU</span>
        <span class="token keyword">float</span> <span class="token operator">*</span> h_output<span class="token punctuation">;</span>
        h_output <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span> <span class="token function">malloc</span><span class="token punctuation">(</span>num_bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// setup our block and grid launch parameters with the dim3 class.</span>
        dim3 <span class="token function">block</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        dim3 <span class="token function">grid</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// launch our kernel</span>
        matrix_mult_ker <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> grid<span class="token punctuation">,</span> block <span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">(</span>d_A<span class="token punctuation">,</span> d_B<span class="token punctuation">,</span> d_output<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// synchronize on the host, to ensure our kernel has finished executing</span>
<span class="token punctuation">.</span>
        <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// copy output from device to host.</span>
        <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_output<span class="token punctuation">,</span> d_output<span class="token punctuation">,</span> num_bytes<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// synchronize again.</span>
        <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// free arrays on device.</span>
        <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_A<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_B<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_output<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// reset the GPU.</span>
        <span class="token function">cudaDeviceReset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>


        <span class="token comment" spellcheck="true">// Check to see if we got the expected output.</span>
        <span class="token comment" spellcheck="true">// in both cases, remember to de-allocate h_output before returning.</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">allclose</span><span class="token punctuation">(</span>h_AxB<span class="token punctuation">,</span> h_output<span class="token punctuation">,</span> N<span class="token operator">*</span>N<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token punctuation">{</span>
                <span class="token function">printf</span><span class="token punctuation">(</span>"Error<span class="token operator">!</span>  Output of kernel does not match expected output
<span class="token punctuation">.</span>\n"<span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token function">free</span><span class="token punctuation">(</span>h_output<span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token keyword">return</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token keyword">else</span>
        <span class="token punctuation">{</span>
                <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Success!  Output of kernel matches expected output.\n"</span><span class="token punctuation">)</span>
<span class="token punctuation">;</span>
                <span class="token function">free</span><span class="token punctuation">(</span>h_output<span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token keyword">return</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>


<span class="token punctuation">}</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><p>本章简单介绍了一下<code>nvprof</code>命令和<code>nsight systems</code>:</p>
<p>nvprof命令使用如下：</p>
<pre class="line-numbers language-bash"><code class="language-bash">$ nvprof ./mc     
<span class="token operator">==</span>27811<span class="token operator">==</span> NVPROF is profiling process 27811, command: ./mc
Success<span class="token operator">!</span>  Output of kernel matches expected output.
<span class="token operator">==</span>27811<span class="token operator">==</span> Profiling application: ./mc
<span class="token operator">==</span>27811<span class="token operator">==</span> Profiling result:
            Type  Time<span class="token punctuation">(</span>%<span class="token punctuation">)</span>      Time     Calls       Avg       Min       Max  Name
 GPU activities:   43.18%  1.8240us         1  1.8240us  1.8240us  1.8240us  matrix_mult_ker<span class="token punctuation">(</span>float*, float*, float*, int<span class="token punctuation">)</span>
                   32.58%  1.3760us         1  1.3760us  1.3760us  1.3760us  <span class="token punctuation">[</span>CUDA memcpy DtoH<span class="token punctuation">]</span>
                   24.24%  1.0240us         2     512ns     352ns     672ns  <span class="token punctuation">[</span>CUDA memcpy HtoD<span class="token punctuation">]</span>
      API calls:   73.57%  57.762ms         1  57.762ms  57.762ms  57.762ms  cudaSetDevice
                   24.33%  19.100ms         1  19.100ms  19.100ms  19.100ms  cudaDeviceReset
                    1.58%  1.2390ms         1  1.2390ms  1.2390ms  1.2390ms  cudaLaunchKernel
                    0.26%  203.03us       114  1.7800us     230ns  80.280us  cuDeviceGetAttribute
                    0.10%  78.900us         3  26.300us  2.6500us  73.320us  cudaMalloc
                    0.09%  73.341us         3  24.447us  2.5900us  66.151us  cudaFree
                    0.04%  31.430us         3  10.476us  4.7200us  13.800us  cudaMemcpy
                    0.01%  10.370us         1  10.370us  10.370us  10.370us  cuDeviceGetName
                    0.01%  4.9200us         1  4.9200us  4.9200us  4.9200us  cuDeviceGetPCIBusId
                    0.01%  4.6700us         2  2.3350us     950ns  3.7200us  cudaDeviceSynchronize
                    0.00%  2.1900us         3     730ns     350ns  1.2000us  cuDeviceGetCount
                    0.00%     720ns         2     360ns     240ns     480ns  cuDeviceGet
                    0.00%     440ns         1     440ns     440ns     440ns  cuModuleGetLoadingMode
                    0.00%     430ns         1     430ns     430ns     430ns  cuDeviceTotalMem
                    0.00%     360ns         1     360ns     360ns     360ns  cuDeviceGetUuid
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以分析出来哪些API比较耗时。</p>
<p><code>nsight systems</code>更为强大，可以给出很多报告，具体耗时分析等内容：</p>
<p> 10 <img src="/images/report.png" alt="alt text" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p> 10 <img src="/images/timeline.png" alt="alt text" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这部分学习主要包括：</p>
<p>1 . 了解了Pycuda的调试方法，包括<code>printf</code>等；</p>
<ol start="2">
<li>学习简单的C语言Cuda实例；</li>
<li>程序的性能分析与报告等。</li>
</ol>
<p>下一部分是Cuda科学计算库的使用。</p>

</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2024/06/17/PyCuda%E7%AC%94%E8%AE%B0%E4%B8%89/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="/img/block.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" #.">
                    未分类
                </a>
            </div>
            <h5>
                <a href="/2024/06/17/PyCuda%E7%AC%94%E8%AE%B0%E4%B8%89/" class="trm-anima-link">
                    PyCuda笔记三
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>24/06/17</li>
                <li>07:44</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-scroll-animation">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.0.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.1.8
            </span>
        </div>
      

     

     
</footer>
 
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

            
<div class="trm-fixed-container">
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    

    

    <!-- 数学公式 -->
    

    <!-- 评论插件 -->
    
        

        
    



<!-- CDN -->


    

    

    




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.1.8"></script>

</body>

</html>