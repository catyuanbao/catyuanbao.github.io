<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="背景本文主要记录《Hands-On GPU Programming with Python and CUDA 》第7章的内容，这里主要介绍了如何使用相关的数学库进行一些简单的数学计算以及相关的数学知识。 主要包括：  cublas 简单线性代数库 cufft 快速傅里叶变换库 cuSolver 复杂的线性代数库  第六章使用cublas计算ax+y这里举例是一个简单的向量运算，ax+y的举例： i">
<meta property="og:type" content="article">
<meta property="og:title" content="PyCuda笔记五">
<meta property="og:url" content="http://example.com/2024/06/17/PyCuda%E7%AC%94%E8%AE%B0%E4%BA%94/index.html">
<meta property="og:site_name" content="Cat YuanBao">
<meta property="og:description" content="背景本文主要记录《Hands-On GPU Programming with Python and CUDA 》第7章的内容，这里主要介绍了如何使用相关的数学库进行一些简单的数学计算以及相关的数学知识。 主要包括：  cublas 简单线性代数库 cufft 快速傅里叶变换库 cuSolver 复杂的线性代数库  第六章使用cublas计算ax+y这里举例是一个简单的向量运算，ax+y的举例： i">
<meta property="og:locale">
<meta property="article:published_time" content="2024-06-17T09:19:13.000Z">
<meta property="article:modified_time" content="2024-06-17T09:19:31.421Z">
<meta property="article:author" content="橘猫元宝">
<meta property="article:tag" content="cat">
<meta name="twitter:card" content="summary">

    <meta name="keywords" content="cat">


<title >PyCuda笔记五</title>

<!-- Favicon -->

    <link href='/img/favicon.svg?v=2.1.8' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/img/favicon.svg?v=2.1.8' rel='icon' type='image/png' sizes='32x32' ></link>




<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    




<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"橘猫元宝","root":"/","typed_text":null,"theme_version":"2.1.8","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"/img/favicon.svg","icon16":"/img/favicon.svg","icon32":"/img/favicon.svg","appleTouchIcon":null,"webmanifest":null,"visibilitychange":false,"hidden":"/failure.ico","showText":"(/≧▽≦/)咦！又好了！","hideText":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://unpkg.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","user_tag":"fas fa-user-alt","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"prismjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2024-06-17 17:19:31"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.1.8" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->
 
<meta name="generator" content="Hexo 7.0.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/img/favicon.svg">
    
    
        <div class="trm-logo-text">
            元宝<span>橘猫</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    归档
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="/img/banner.png">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            NEWS LETTER
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            PyCuda笔记五
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2024
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/img/cat.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        橘猫元宝
    </h5>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com" title="Github" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                Residence:
            </div>
            <div class="trm-label trm-label-light">
                Mars
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            06/17
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            17:19
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            橘猫元宝
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>本文主要记录《Hands-On GPU Programming with Python and CUDA 》第7章的内容，这里主要介绍了如何使用相关的数学库进行一些简单的数学计算以及相关的数学知识。</p>
<p>主要包括：</p>
<ol>
<li><code>cublas</code> 简单线性代数库</li>
<li><code>cufft</code> 快速傅里叶变换库</li>
<li><code>cuSolver</code> 复杂的线性代数库</li>
</ol>
<h3 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h3><h4 id="使用cublas"><a href="#使用cublas" class="headerlink" title="使用cublas"></a>使用cublas</h4><h4 id="计算ax-y"><a href="#计算ax-y" class="headerlink" title="计算ax+y"></a>计算ax+y</h4><p>这里举例是一个简单的向量运算，ax+y的举例：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit
<span class="token keyword">from</span> pycuda <span class="token keyword">import</span> gpuarray
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">from</span> skcuda <span class="token keyword">import</span> cublas

a <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

x_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
y_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># cublas实例</span>
cublas_context_h <span class="token operator">=</span> cublas<span class="token punctuation">.</span>cublasCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

cublas<span class="token punctuation">.</span>cublasSaxpy<span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">,</span> x_gpu<span class="token punctuation">.</span>size<span class="token punctuation">,</span> a<span class="token punctuation">,</span> x_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span>
                   y_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y_gpu<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 销毁实例</span>
cublas<span class="token punctuation">.</span>cublasDestroy<span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'this is close to the numpy approximation'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>allclose<span class="token punctuation">(</span>a<span class="token operator">*</span>x <span class="token operator">+</span> y<span class="token punctuation">,</span> y_gpu<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>cublasSaxpy S表示单精度，32位浮点数</p>
<p>cublasDaxpy D表示双精度，64位浮点数</p>
<p>cublasCaxpy C表示<code>complex64</code></p>
<p>cublasZaxpy Z表示<code>complex128</code></p>
<p>通过函数的命名规则，可以判断出来函数的适用类型。</p>
<h4 id="计算点积"><a href="#计算点积" class="headerlink" title="计算点积"></a>计算点积</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit
<span class="token keyword">from</span> pycuda <span class="token keyword">import</span> gpuarray
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">from</span> skcuda <span class="token keyword">import</span> cublas

a <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

x_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
y_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># cublas实例</span>
cublas_context_h <span class="token operator">=</span> cublas<span class="token punctuation">.</span>cublasCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

dot_res <span class="token operator">=</span> cublas<span class="token punctuation">.</span>cublasSdot<span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">,</span> x_gpu<span class="token punctuation">.</span>size<span class="token punctuation">,</span>  x_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span>
                   y_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">)</span>

cublas<span class="token punctuation">.</span>cublasDestroy<span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dot_res<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># -14 = -1 + (-4) + (-9)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="计算向量L2的范数"><a href="#计算向量L2的范数" class="headerlink" title="计算向量L2的范数"></a>计算向量L2的范数</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit
<span class="token keyword">from</span> pycuda <span class="token keyword">import</span> gpuarray
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">from</span> skcuda <span class="token keyword">import</span> cublas

a <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

x_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
y_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># cublas实例</span>
cublas_context_h <span class="token operator">=</span> cublas<span class="token punctuation">.</span>cublasCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

l2_res <span class="token operator">=</span> cublas<span class="token punctuation">.</span>cublasSnrm2<span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">,</span> x_gpu<span class="token punctuation">.</span>size<span class="token punctuation">,</span>  x_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">)</span>

cublas<span class="token punctuation">.</span>cublasDestroy<span class="token punctuation">(</span>cublas_context_h<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>l2_res<span class="token punctuation">)</span> 
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="GEMM运算"><a href="#GEMM运算" class="headerlink" title="GEMM运算"></a>GEMM运算</h4><p>GEMM（General Matrix Multiply）是线性代数中矩阵乘法运算的一种重要形式，全称为通用矩阵乘法。</p>
<p>GEMM运算主要用于将两个矩阵相乘，并且可以选择性地加上一个第三矩阵。它在许多科学计算、工程和机器学习应用中扮演着关键角色。</p>
<p>具体步骤为：</p>
<ol>
<li>计算矩阵 A 和矩阵 B 的乘积。</li>
<li>乘积结果乘以标量 α。</li>
<li>将结果与矩阵 C 乘以标量 β 相加。</li>
</ol>
<p>实例如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment" spellcheck="true"># 定义矩阵 A, B, C 和标量 alpha, beta</span>
A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float<span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float<span class="token punctuation">)</span>
C <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float<span class="token punctuation">)</span>
alpha <span class="token operator">=</span> <span class="token number">1.0</span>
beta <span class="token operator">=</span> <span class="token number">1.0</span>

<span class="token comment" spellcheck="true"># 计算 GEMM 运算</span>
C <span class="token operator">=</span> alpha <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span> <span class="token operator">+</span> beta <span class="token operator">*</span> C

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"结果矩阵 C:"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>C<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用这个算法可以用来测量GPU的性能，代码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit
<span class="token keyword">from</span> pycuda <span class="token keyword">import</span> gpuarray
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> skcuda <span class="token keyword">import</span> cublas
<span class="token keyword">from</span> time <span class="token keyword">import</span> time

m <span class="token operator">=</span> <span class="token number">5000</span>
n <span class="token operator">=</span> <span class="token number">10000</span>
k <span class="token operator">=</span> <span class="token number">10000</span>


<span class="token keyword">def</span> <span class="token function">compute_gflops</span><span class="token punctuation">(</span>precision<span class="token operator">=</span><span class="token string">'S'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>


    <span class="token keyword">if</span> precision<span class="token operator">==</span><span class="token string">'S'</span><span class="token punctuation">:</span>
        float_type <span class="token operator">=</span> <span class="token string">'float32'</span>
    <span class="token keyword">elif</span> precision<span class="token operator">==</span><span class="token string">'D'</span><span class="token punctuation">:</span>
        float_type <span class="token operator">=</span> <span class="token string">'float64'</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span>
        
        
    A <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>float_type<span class="token punctuation">)</span>
    B <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>k<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>float_type<span class="token punctuation">)</span>
    C <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>float_type<span class="token punctuation">)</span>

    A_cm <span class="token operator">=</span> A<span class="token punctuation">.</span>T<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    B_cm <span class="token operator">=</span> B<span class="token punctuation">.</span>T<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    C_cm <span class="token operator">=</span> C<span class="token punctuation">.</span>T<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

    A_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>A_cm<span class="token punctuation">)</span>
    B_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>B_cm<span class="token punctuation">)</span>
    C_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>C_cm<span class="token punctuation">)</span>

    alpha <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span>
    beta <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span>

    transa <span class="token operator">=</span> cublas<span class="token punctuation">.</span>_CUBLAS_OP<span class="token punctuation">[</span><span class="token string">'N'</span><span class="token punctuation">]</span>
    transb <span class="token operator">=</span> cublas<span class="token punctuation">.</span>_CUBLAS_OP<span class="token punctuation">[</span><span class="token string">'N'</span><span class="token punctuation">]</span>

    lda <span class="token operator">=</span> m
    ldb <span class="token operator">=</span> k
    ldc <span class="token operator">=</span> m

    t <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    handle <span class="token operator">=</span> cublas<span class="token punctuation">.</span>cublasCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

    
    <span class="token keyword">exec</span><span class="token punctuation">(</span>'cublas<span class="token punctuation">.</span>cublas<span class="token operator">%</span>sgemm<span class="token punctuation">(</span>handle<span class="token punctuation">,</span> transa<span class="token punctuation">,</span> transb<span class="token punctuation">,</span> m<span class="token punctuation">,</span> n<span class="token punctuation">,</span> k<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> A_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> lda<span class="token punctuation">,</span> \
                        B_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> ldb<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> C_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">,</span> ldc<span class="token punctuation">)</span>' <span class="token operator">%</span> precision<span class="token punctuation">)</span>
    
    cublas<span class="token punctuation">.</span>cublasDestroy<span class="token punctuation">(</span>handle<span class="token punctuation">)</span>
    t <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> t

    gflops <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>m<span class="token operator">*</span>n<span class="token operator">*</span><span class="token punctuation">(</span>k<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token operator">**</span><span class="token operator">-</span><span class="token number">9</span><span class="token punctuation">)</span> <span class="token operator">/</span> t 
    
    <span class="token keyword">return</span> gflops

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span> <span class="token string">'Single-precision performance: %s GFLOPS'</span> <span class="token operator">%</span> compute_gflops<span class="token punctuation">(</span><span class="token string">'S'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span> <span class="token string">'Double-precision performance: %s GFLOPS'</span> <span class="token operator">%</span> compute_gflops<span class="token punctuation">(</span><span class="token string">'D'</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>输出如下：</p>
<pre><code>Single-precision performance: 3231.699578040868 GFLOPS
Double-precision performance: 165.57756930075516 GFLOPS
</code></pre>
<p>和网上给的1660显卡的性能有点出入，可能这个方法不太准确。</p>
<pre class="line-numbers language-bash"><code class="language-bash">Single Precision performance, 5026.56 GFLOPS
Double Precision performance, 157.08 GFLOPS
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="一维快速傅里叶变换实例"><a href="#一维快速傅里叶变换实例" class="headerlink" title="一维快速傅里叶变换实例"></a>一维快速傅里叶变换实例</h4><p>快速傅里叶变换（Fast Fourier Transform，简称 FFT）是一种高效计算离散傅里叶变换（Discrete Fourier Transform，简称 DFT）及其逆变换的方法。</p>
<p>FFT 是许多信号处理、图像处理和科学计算应用中的基本工具，因为它可以将时间或空间域中的信号转换为频率域表示，从而揭示信号的频率成分。</p>
<p>实例如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit
<span class="token keyword">from</span> pycuda <span class="token keyword">import</span> gpuarray
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> skcuda <span class="token keyword">import</span> fft

<span class="token comment" spellcheck="true"># 生成随机数据</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
x_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 准备存放结果的数组</span>
x_hat <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">1000</span> <span class="token operator">//</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>complex64<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 注意复数结果的长度</span>

<span class="token comment" spellcheck="true"># 正向FFT计划</span>
plan <span class="token operator">=</span> fft<span class="token punctuation">.</span>Plan<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> np<span class="token punctuation">.</span>complex64<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 执行正向FFT</span>
fft<span class="token punctuation">.</span>fft<span class="token punctuation">(</span>x_gpu<span class="token punctuation">,</span> x_hat<span class="token punctuation">,</span> plan<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 准备逆向FFT的存放数组</span>
x_inv <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>x_gpu<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 逆向FFT计划</span>
inverse_plan <span class="token operator">=</span> fft<span class="token punctuation">.</span>Plan<span class="token punctuation">(</span>x_hat<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> np<span class="token punctuation">.</span>complex64<span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 执行逆向FFT</span>
fft<span class="token punctuation">.</span>ifft<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x_inv<span class="token punctuation">,</span> inverse_plan<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 由于fft.ifft没有scale参数，手动缩放结果</span>
x_inv <span class="token operator">*=</span> <span class="token number">1.0</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># 使用numpy进行正向FFT</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>fft<span class="token punctuation">.</span>rfft<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 使用numpy进行逆向FFT</span>
y_inv <span class="token operator">=</span> np<span class="token punctuation">.</span>fft<span class="token punctuation">.</span>irfft<span class="token punctuation">(</span>y<span class="token punctuation">,</span> n<span class="token operator">=</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 验证结果</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'cufft matches numpy fft:'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>allclose<span class="token punctuation">(</span>x_hat<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'cufft inverse matches original x:'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>allclose<span class="token punctuation">(</span>x_inv<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 释放CUDA上下文</span>
<span class="token keyword">del</span> plan
<span class="token keyword">del</span> inverse_plan
<span class="token keyword">del</span> x_gpu
<span class="token keyword">del</span> x_hat
<span class="token keyword">del</span> x_inv
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>原书的代码有点问题，运行结果如下：</p>
<pre class="line-numbers language-bash"><code class="language-bash">$ python3 fft_ex.py
/home/yuanbao/.local/lib/python3.8/site-packages/skcuda/cublas.py:284: UserWarning: creating CUBLAS context to get version number
  warnings.warn<span class="token punctuation">(</span><span class="token string">'creating CUBLAS context to get version number'</span><span class="token punctuation">)</span>
cufft matches numpy fft: True
cufft inverse matches original x: False
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这里第二个结果是<code>False</code>,中间的原理不太懂，先略过。</p>
<h4 id="cuSolver"><a href="#cuSolver" class="headerlink" title="cuSolver"></a>cuSolver</h4><p>实例如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit
<span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>gpuarray <span class="token keyword">as</span> gpuarray
<span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>driver <span class="token keyword">as</span> drv
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> skcuda <span class="token keyword">import</span> cusolver<span class="token punctuation">,</span> cublas<span class="token punctuation">,</span> linalg

<span class="token comment" spellcheck="true"># 初始化</span>
linalg<span class="token punctuation">.</span>init<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 定义矩阵</span>
A <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 将矩阵传到GPU上</span>
A_gpu <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>to_gpu<span class="token punctuation">(</span>A<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 准备cuSolver和cublas的handle</span>
cusolver_handle <span class="token operator">=</span> cusolver<span class="token punctuation">.</span>cusolverDnCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
cublas_handle <span class="token operator">=</span> cublas<span class="token punctuation">.</span>cublasCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 参数设置</span>
m<span class="token punctuation">,</span> n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape
lda <span class="token operator">=</span> max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span>
S <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>min<span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
U <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
Vt <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
work <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 工作空间的大小稍后会重新分配</span>
dev_info <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
rwork <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">7</span><span class="token operator">*</span>min<span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 分配临时存储空间</span>

<span class="token comment" spellcheck="true"># 查询工作空间大小</span>
lwork <span class="token operator">=</span> cusolver<span class="token punctuation">.</span>cusolverDnSgesvd_bufferSize<span class="token punctuation">(</span>cusolver_handle<span class="token punctuation">,</span> m<span class="token punctuation">,</span> n<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 分配工作空间</span>
work <span class="token operator">=</span> gpuarray<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>lwork<span class="token punctuation">,</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># SVD分解</span>
jobu <span class="token operator">=</span> <span class="token string">'A'</span>  <span class="token comment" spellcheck="true"># 全矩阵U</span>
jobvt <span class="token operator">=</span> <span class="token string">'A'</span>  <span class="token comment" spellcheck="true"># 全矩阵Vt</span>
cusolver<span class="token punctuation">.</span>cusolverDnSgesvd<span class="token punctuation">(</span>cusolver_handle<span class="token punctuation">,</span> jobu<span class="token punctuation">,</span> jobvt<span class="token punctuation">,</span> m<span class="token punctuation">,</span> n<span class="token punctuation">,</span> int<span class="token punctuation">(</span>A_gpu<span class="token punctuation">.</span>gpudata<span class="token punctuation">)</span><span class="token punctuation">,</span> lda<span class="token punctuation">,</span>
                          int<span class="token punctuation">(</span>S<span class="token punctuation">.</span>gpudata<span class="token punctuation">)</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>U<span class="token punctuation">.</span>gpudata<span class="token punctuation">)</span><span class="token punctuation">,</span> m<span class="token punctuation">,</span> int<span class="token punctuation">(</span>Vt<span class="token punctuation">.</span>gpudata<span class="token punctuation">)</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> 
                          int<span class="token punctuation">(</span>work<span class="token punctuation">.</span>gpudata<span class="token punctuation">)</span><span class="token punctuation">,</span> lwork<span class="token punctuation">,</span> int<span class="token punctuation">(</span>rwork<span class="token punctuation">.</span>gpudata<span class="token punctuation">)</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>dev_info<span class="token punctuation">.</span>gpudata<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 将结果从GPU传回CPU</span>
S_cpu <span class="token operator">=</span> S<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
U_cpu <span class="token operator">=</span> U<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
Vt_cpu <span class="token operator">=</span> Vt<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 创建一个对角矩阵S</span>
S_diag <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
S_diag<span class="token punctuation">[</span><span class="token punctuation">:</span>min<span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>min<span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>S_cpu<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Original matrix A:\n"</span><span class="token punctuation">,</span> A<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nSingular values:\n"</span><span class="token punctuation">,</span> S_cpu<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nLeft singular vectors (U):\n"</span><span class="token punctuation">,</span> U_cpu<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nRight singular vectors (Vt):\n"</span><span class="token punctuation">,</span> Vt_cpu<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 验证结果是否一致</span>
A_reconstructed <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>U_cpu<span class="token punctuation">,</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>S_diag<span class="token punctuation">,</span> Vt_cpu<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nReconstructed matrix A:\n"</span><span class="token punctuation">,</span> A_reconstructed<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Reconstruction error:"</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>A <span class="token operator">-</span> A_reconstructed<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 清理</span>
cusolver<span class="token punctuation">.</span>cusolverDnDestroy<span class="token punctuation">(</span>cusolver_handle<span class="token punctuation">)</span>
cublas<span class="token punctuation">.</span>cublasDestroy<span class="token punctuation">(</span>cublas_handle<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>运行结果：</p>
<pre class="line-numbers language-bash"><code class="language-bash"> <span class="token punctuation">[</span><span class="token punctuation">[</span> 0.9507715  -1.2778851  -0.26034012<span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 2.4607348   0.51366657 -0.08188383<span class="token punctuation">]</span>
 <span class="token punctuation">[</span>-0.15270862  1.7176826   1.6454922 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span>-0.7959227  -0.31905887  0.1854843 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 1.4299569  -0.695057   -1.5917972 <span class="token punctuation">]</span><span class="token punctuation">]</span>

Singular values:
 <span class="token punctuation">[</span>3.411056   2.8916128  0.66826373<span class="token punctuation">]</span>

Left singular vectors <span class="token punctuation">(</span>U<span class="token punctuation">)</span>:
 <span class="token punctuation">[</span><span class="token punctuation">[</span>-0.26109338  0.35919553 -0.00233802 -0.87721765 -0.18246181<span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 0.12404129 -0.04332143 -0.7763218  -0.17551318  0.59097904<span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 0.45052654 -0.62387836 -0.23035657 -0.27954894 -0.52591825<span class="token punctuation">]</span>
 <span class="token punctuation">[</span>-0.8360746  -0.5205016  -0.15730858  0.04761273 -0.0551741 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 0.12020408 -0.45711663  0.5652509  -0.34535423  0.5812205 <span class="token punctuation">]</span><span class="token punctuation">]</span>

Right singular vectors <span class="token punctuation">(</span>Vt<span class="token punctuation">)</span>:
 <span class="token punctuation">[</span><span class="token punctuation">[</span>-0.8674636   0.08544619  0.49010798<span class="token punctuation">]</span>
 <span class="token punctuation">[</span>-0.39158502 -0.7249229  -0.5666993 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 0.30686817 -0.68351     0.6623036 <span class="token punctuation">]</span><span class="token punctuation">]</span>

Reconstructed matrix A:
 <span class="token punctuation">[</span><span class="token punctuation">[</span> 0.3653657  -0.8279752  -1.0261316 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span>-0.47718012  0.4815601  -0.06523477<span class="token punctuation">]</span>
 <span class="token punctuation">[</span>-0.673907    1.5443016   1.6735634 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 3.0310285   0.91924286 -0.6144284 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> 0.27783427  0.7350544   1.2001973 <span class="token punctuation">]</span><span class="token punctuation">]</span>
Reconstruction error: 6.16874
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这部分学习主要包括：</p>
<p>1 . 了解了Pycuda的cublas和cufft以及cuSolver;</p>
<ol start="2">
<li>了解了傅里叶变换的基本知识。</li>
</ol>
<p>这部分学习的效果一般，主要是相关的数学背景知识不足，有机会再仔细了解下内部原理，下一部分是PyCuda设备函数库和数学库的使用。</p>

</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2024/06/17/Python%E5%A6%82%E4%BD%95%E5%88%B6%E4%BD%9C%E7%AE%80%E5%8D%95PPT/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="/img/block.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" #.">
                    未分类
                </a>
            </div>
            <h5>
                <a href="/2024/06/17/Python%E5%A6%82%E4%BD%95%E5%88%B6%E4%BD%9C%E7%AE%80%E5%8D%95PPT/" class="trm-anima-link">
                    Python如何制作简单PPT
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>24/06/17</li>
                <li>15:37</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-scroll-animation">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.0.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.1.8
            </span>
        </div>
      

     

     
</footer>
 
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

            
<div class="trm-fixed-container">
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    

    

    <!-- 数学公式 -->
    

    <!-- 评论插件 -->
    
        

        
    



<!-- CDN -->


    

    

    




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.1.8"></script>

</body>

</html>